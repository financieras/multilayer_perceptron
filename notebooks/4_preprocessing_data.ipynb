{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bcd850-5e4f-49e5-b918-029809d0b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de Datos para Perceptrón Multicapa\n",
    "# ========================================\n",
    "# Este notebook se encarga de:\n",
    "# 1. Cargar los datos limpios (no normalizados)\n",
    "# 2. Dividir el conjunto de datos en:\n",
    "#    - Conjunto de entrenamiento (60%)\n",
    "#    - Conjunto de validación (20%)\n",
    "#    - Conjunto de prueba (20%)\n",
    "# 3. Normalizar los datos usando estadísticas SOLO del conjunto de entrenamiento\n",
    "# 4. Guardar los conjuntos normalizados y los parámetros de normalización\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configura el estilo de visualización\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Añadir el directorio raíz al path para poder importar módulos personalizados\n",
    "sys.path.append('..')\n",
    "from utils.config import FEATURE_NAMES\n",
    "\n",
    "# Crear las carpetas de output si no existen\n",
    "for folder in ['../output/models', '../output/figures']:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 1. Carga de datos limpios (no normalizados)\n",
    "# ----------------------------------\n",
    "print(\"Cargando datos limpios (no normalizados)...\")\n",
    "df = pd.read_csv('../data/processed/cleaned_data.csv')\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"Primeras 5 filas del dataset:\")\n",
    "df.head()\n",
    "\n",
    "# 2. Análisis rápido de los datos cargados\n",
    "# ----------------------------------------\n",
    "print(\"\\nInformación del dataset:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "df.describe().T\n",
    "\n",
    "# Verificar que no hay valores nulos\n",
    "print(\"\\nVerificando valores nulos:\")\n",
    "print(df.isnull().sum().sum())\n",
    "\n",
    "# Mostrar la distribución de clases\n",
    "print(\"\\nDistribución de clases (diagnóstico):\")\n",
    "class_counts = df['diagnosis'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='diagnosis', data=df, palette='Set1')\n",
    "plt.title('Distribución de Diagnósticos')\n",
    "plt.xlabel('Diagnóstico (0=Benigno, 1=Maligno)')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.savefig('../output/figures/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Dividir los datos en conjuntos de entrenamiento, validación y prueba\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"\\nDividiendo los datos en conjuntos de entrenamiento, validación y prueba...\")\n",
    "\n",
    "# Separar características (X) y etiquetas (y)\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Primera división: 80% para entrenamiento+validación, 20% para prueba\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Segunda división: 75% del conjunto temporal para entrenamiento (60% del total), \n",
    "# 25% para validación (20% del total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Tamaño conjunto de entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/df.shape[0]:.1%} del total)\")\n",
    "print(f\"Tamaño conjunto de validación: {X_val.shape[0]} muestras ({X_val.shape[0]/df.shape[0]:.1%} del total)\")\n",
    "print(f\"Tamaño conjunto de prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/df.shape[0]:.1%} del total)\")\n",
    "\n",
    "# Verificar la distribución de clases en cada conjunto\n",
    "print(\"\\nDistribución de clases en cada conjunto:\")\n",
    "print(f\"Entrenamiento: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Validación: {y_val.value_counts().to_dict()}\")\n",
    "print(f\"Prueba: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Gráfico de distribución de clases por conjunto\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts = pd.DataFrame({\n",
    "    'Entrenamiento': y_train.value_counts(),\n",
    "    'Validación': y_val.value_counts(),\n",
    "    'Prueba': y_test.value_counts()\n",
    "})\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title('Distribución de Clases por Conjunto')\n",
    "plt.xlabel('Diagnóstico (0=Benigno, 1=Maligno)')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/class_distribution_by_set.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 4. Normalización de datos (Z-score)\n",
    "# -----------------------------------\n",
    "print(\"\\nNormalizando datos con Z-score usando SOLO estadísticas del conjunto de entrenamiento...\")\n",
    "\n",
    "# Inicializar el normalizador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar el normalizador SOLO a los datos de entrenamiento\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transformar todos los conjuntos usando las estadísticas del conjunto de entrenamiento\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Crear DataFrames con los datos normalizados para análisis\n",
    "X_train_norm = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_val_norm = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "X_test_norm = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Guardar medias y desviaciones estándar para uso futuro\n",
    "normalization_params = {\n",
    "    'means': {col: float(scaler.mean_[i]) for i, col in enumerate(X_train.columns)},\n",
    "    'stds': {col: float(scaler.scale_[i]) for i, col in enumerate(X_train.columns)}\n",
    "}\n",
    "\n",
    "with open('../output/normalization_params.json', 'w') as f:\n",
    "    json.dump(normalization_params, f, indent=4)\n",
    "print(f\"Guardados parámetros de normalización en ../output/normalization_params.json\")\n",
    "\n",
    "# Visualizar efecto de la normalización en algunas características\n",
    "print(\"\\nEfecto de la normalización (datos de entrenamiento):\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "selected_features = ['f01', 'f02', 'f07', 'f08']  # Importantes según análisis previo\n",
    "\n",
    "for i, feature in enumerate(selected_features):\n",
    "    # Datos originales\n",
    "    sns.histplot(X_train[feature], color='blue', alpha=0.5, \n",
    "                 label='Original', ax=axs[i], kde=True)\n",
    "    \n",
    "    # Datos normalizados\n",
    "    sns.histplot(X_train_norm[feature], color='red', alpha=0.5, \n",
    "                 label='Normalizado', ax=axs[i], kde=True)\n",
    "    \n",
    "    axs[i].set_title(f'{feature} ({FEATURE_NAMES[feature]})')\n",
    "    axs[i].axvline(x=0, color='black', linestyle='--', alpha=0.7)\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/normalization_effect.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 5. Preparar los datos para el modelo\n",
    "# -----------------------------------\n",
    "# Para un perceptrón multicapa sin usar frameworks como TensorFlow o PyTorch,\n",
    "# convertimos los datos a arrays NumPy\n",
    "\n",
    "# Conversión a arrays NumPy\n",
    "X_train_array = X_train_scaled\n",
    "X_val_array = X_val_scaled\n",
    "X_test_array = X_test_scaled\n",
    "\n",
    "y_train_array = y_train.to_numpy().reshape(-1, 1)  # Reshape para tener la dimensión correcta\n",
    "y_val_array = y_val.to_numpy().reshape(-1, 1)\n",
    "y_test_array = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "print(f\"\\nForma de los arrays de características:\")\n",
    "print(f\"X_train: {X_train_array.shape}\")\n",
    "print(f\"X_val: {X_val_array.shape}\")\n",
    "print(f\"X_test: {X_test_array.shape}\")\n",
    "\n",
    "print(f\"\\nForma de los arrays de etiquetas:\")\n",
    "print(f\"y_train: {y_train_array.shape}\")\n",
    "print(f\"y_val: {y_val_array.shape}\")\n",
    "print(f\"y_test: {y_test_array.shape}\")\n",
    "\n",
    "# 6. Guardar los conjuntos de datos procesados\n",
    "# -------------------------------------------\n",
    "# Guardar como CSV para uso posterior\n",
    "print(\"\\nGuardando conjuntos de datos procesados...\")\n",
    "\n",
    "# Guardar conjuntos de entrenamiento, validación y prueba normalizados\n",
    "datasets_norm = {\n",
    "    'train': (X_train_norm, y_train),\n",
    "    'val': (X_val_norm, y_val),\n",
    "    'test': (X_test_norm, y_test)\n",
    "}\n",
    "\n",
    "for name, (X_set, y_set) in datasets_norm.items():\n",
    "    # Crear un DataFrame con características y etiqueta\n",
    "    combined_df = X_set.copy()\n",
    "    combined_df['diagnosis'] = y_set.values\n",
    "    \n",
    "    # Guardar en CSV\n",
    "    filepath = f'../data/processed/{name}_set_normalized.csv'\n",
    "    combined_df.to_csv(filepath, index=False)\n",
    "    print(f\"Guardado conjunto normalizado {name} en {filepath}\")\n",
    "\n",
    "# Guardar los parámetros de división para reproducibilidad\n",
    "split_params = {\n",
    "    'train_size': X_train.shape[0],\n",
    "    'val_size': X_val.shape[0],\n",
    "    'test_size': X_test.shape[0],\n",
    "    'train_pct': float(X_train.shape[0]/df.shape[0]),\n",
    "    'val_pct': float(X_val.shape[0]/df.shape[0]),\n",
    "    'test_pct': float(X_test.shape[0]/df.shape[0]),\n",
    "    'stratify': True,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "with open('../output/split_params.json', 'w') as f:\n",
    "    json.dump(split_params, f, indent=4)\n",
    "print(f\"Guardados parámetros de división en ../output/split_params.json\")\n",
    "\n",
    "# 7. Guardar arrays NumPy para uso directo en el modelo\n",
    "# Estos archivos son más eficientes para cargar directamente en el modelo\n",
    "print(\"\\nGuardando arrays NumPy para uso directo en el modelo...\")\n",
    "np.save('../data/processed/X_train.npy', X_train_array)\n",
    "np.save('../data/processed/y_train.npy', y_train_array)\n",
    "np.save('../data/processed/X_val.npy', X_val_array)\n",
    "np.save('../data/processed/y_val.npy', y_val_array)\n",
    "np.save('../data/processed/X_test.npy', X_test_array)\n",
    "np.save('../data/processed/y_test.npy', y_test_array)\n",
    "print(\"Arrays NumPy guardados correctamente.\")\n",
    "\n",
    "# 8. Verificación de la distribución de características normalizadas\n",
    "# ---------------------------------------------------\n",
    "print(\"\\nVerificando la distribución de características normalizadas entre conjuntos...\")\n",
    "\n",
    "# Seleccionar algunas características importantes basadas en análisis previo\n",
    "important_features = ['f01', 'f08', 'f24', 'f28']\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, feature in enumerate(important_features):\n",
    "    sns.kdeplot(X_train_norm[feature], ax=axs[i], label='Entrenamiento', color='blue')\n",
    "    sns.kdeplot(X_val_norm[feature], ax=axs[i], label='Validación', color='green')\n",
    "    sns.kdeplot(X_test_norm[feature], ax=axs[i], label='Prueba', color='red')\n",
    "    \n",
    "    axs[i].set_title(f'Distribución de {feature} normalizado ({FEATURE_NAMES[feature]})')\n",
    "    axs[i].set_xlabel('Valor normalizado')\n",
    "    axs[i].set_ylabel('Densidad')\n",
    "    axs[i].axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/feature_distribution_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 9. Resumen del preprocesamiento\n",
    "# -----------------------------\n",
    "print(\"\\n=== Resumen del Preprocesamiento ===\")\n",
    "print(f\"Dataset original: {df.shape[0]} muestras, {df.shape[1]} características\")\n",
    "print(\"\\nDivisión de datos:\")\n",
    "print(f\"- Entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/df.shape[0]:.1%})\")\n",
    "print(f\"- Validación: {X_val.shape[0]} muestras ({X_val.shape[0]/df.shape[0]:.1%})\")\n",
    "print(f\"- Prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/df.shape[0]:.1%})\")\n",
    "\n",
    "print(\"\\nNormalización:\")\n",
    "print(\"- Método: Z-score (StandardScaler)\")\n",
    "print(\"- Estadísticas calculadas SOLO con datos de entrenamiento\")\n",
    "print(\"- Mismo scaler aplicado a todos los conjuntos\")\n",
    "\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(\"- CSVs con conjuntos normalizados: train_set_normalized.csv, val_set_normalized.csv, test_set_normalized.csv\")\n",
    "print(\"- Arrays NumPy: X_train.npy, y_train.npy, X_val.npy, y_val.npy, X_test.npy, y_test.npy\")\n",
    "print(\"- Parámetros de normalización: normalization_params.json\")\n",
    "print(\"- Parámetros de división: split_params.json\")\n",
    "\n",
    "print(\"\\nPróximos pasos:\")\n",
    "print(\"1. Desarrollar la arquitectura del perceptrón multicapa\")\n",
    "print(\"2. Implementar el algoritmo de entrenamiento\")\n",
    "print(\"3. Entrenar el modelo con los datos de entrenamiento\")\n",
    "print(\"4. Validar el modelo con los datos de validación\")\n",
    "print(\"5. Evaluar el rendimiento final con los datos de prueba\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
